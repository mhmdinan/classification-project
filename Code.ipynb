{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551e647a",
   "metadata": {},
   "source": [
    "## AI CEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b491676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "#First we will import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearnex import patch_sklearn # Imported to use Intel proccessor optimizations for scikit-learn library\n",
    "patch_sklearn()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from codecarbon import EmissionsTracker\n",
    "import os\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of missing entries in dataset: session_id                     0\n",
      "DateTime                       0\n",
      "user_id                        0\n",
      "product                        0\n",
      "campaign_id                    0\n",
      "webpage_id                     0\n",
      "product_category_1             0\n",
      "product_category_2        365854\n",
      "user_group_id              18243\n",
      "gender                     18243\n",
      "age_level                  18243\n",
      "user_depth                 18243\n",
      "city_development_index    125129\n",
      "var_1                          0\n",
      "is_click                       0\n",
      "dtype: int64\n",
      "amount of missing entries in dataset after removing missing entries: session_id            0\n",
      "user_id               0\n",
      "product               0\n",
      "campaign_id           0\n",
      "webpage_id            0\n",
      "product_category_1    0\n",
      "user_group_id         0\n",
      "gender                0\n",
      "age_level             0\n",
      "user_depth            0\n",
      "var_1                 0\n",
      "is_click              0\n",
      "dtype: int64\n",
      "Class distribution before undersampling: Counter({0: 414991, 1: 30057})\n",
      "Class distribution after undersampling: Counter({0: 30057, 1: 30057})\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading and preprcessing steps:\n",
    "try:\n",
    "    df = pd.read_csv(\"Ad_click_prediction_train (1).csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Ad_click_prediction_train (1).csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "num_missing = df.isnull().sum()\n",
    "print(f\"amount of missing entries in dataset: {num_missing}\")\n",
    "#dropping prodduct_category_2 and city development_index due to high missing entries\n",
    "df = df.drop('city_development_index', axis=1)\n",
    "df = df.drop('product_category_2', axis=1)\n",
    "df = df.drop('DateTime', axis = 1)\n",
    "df = df.dropna() #excluding lines with missing entries\n",
    "\n",
    "num_missing = df.isnull().sum()\n",
    "print(f\"amount of missing entries in dataset after removing missing entries: {num_missing}\")\n",
    "\n",
    "LE = LabelEncoder()\n",
    "df['gender'] = LE.fit_transform(df['gender'])\n",
    "df['product'] = LE.fit_transform(df['product'])\n",
    "df.info()\n",
    "\n",
    "x = df.drop('is_click', axis=1)\n",
    "y = df['is_click']\n",
    "\n",
    "# class distribution at start\n",
    "print(\"Class distribution before undersampling:\", Counter(y))\n",
    "\n",
    "# using undersampling to redduce dataset size and make it easier to process compared to oversampling\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=90)\n",
    "\n",
    "# performing undersampling to x and y dataframes;\n",
    "x_undersample, y_undersampled = undersampler.fit_resample(x, y)\n",
    "\n",
    "# class distribution after undersampling:\n",
    "print(\"Class distribution after undersampling:\", Counter(y_undersampled))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dataset paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up classification recording:\n",
    "def evaluate_classifier(name, model, x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test, output_dir=\"emission_logs\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    emtracker = EmissionsTracker(project_name=name, output_dir=output_dir, log_level='ERROR')\n",
    "\n",
    "    emtracker.start_task(f\"Training model using {name}\")\n",
    "    training_start_time = time.time()\n",
    "    model.fit(x_train, y_train)\n",
    "    training_time = time.time() - training_start_time\n",
    "    training_emissions = emtracker.stop_task()\n",
    "\n",
    "    emtracker.start_task(f\"Inference on trained model using {name}\")\n",
    "    infernece_start_time = time.time()\n",
    "    y_pred = model.predict(x_test)\n",
    "    inference_time = time.time() - infernece_start_time\n",
    "    inference_emissions = emtracker.stop_task()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Training time: {training_time}\")\n",
    "    print(f\"Training emissions: {training_emissions.emissions}\")\n",
    "    print(f\"Inference time: {inference_time}\")\n",
    "    print(f\"Inference emissions: {inference_emissions.emissions}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_score,\n",
    "        'Training Time (s)': training_time,\n",
    "        'TrainingeEmissions ': training_emissions.emissions,\n",
    "        'Inference Time (s)' : inference_time,\n",
    "        'Inference emissions' : inference_emissions.emissions\n",
    "    }\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "classifiers_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9225b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Training time: 0.2197582721710205\n",
      "Training emissions: 1.5576464165983525e-06\n",
      "Inference time: 0.002202749252319336\n",
      "Inference emissions: 1.0266600371353927e-06\n",
      "Accuracy: 0.9318\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Model: KNN\n",
      "Training time: 0.2424306869506836\n",
      "Training emissions: 1.5085905897395994e-06\n",
      "Inference time: 3.9327476024627686\n",
      "Inference emissions: 3.322114067437935e-05\n",
      "Accuracy: 0.9286\n",
      "Precision: 0.1103\n",
      "Recall: 0.0066\n",
      "F1 Score: 0.0124\n",
      "Model: NaiveBayes\n",
      "Training time: 0.05916929244995117\n",
      "Training emissions: 1.1418168105125682e-06\n",
      "Inference time: 0.01430201530456543\n",
      "Inference emissions: 1.0500865484389465e-06\n",
      "Accuracy: 0.9318\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "#implementing classifiers\n",
    "for name, model in classifiers.items():\n",
    "    evaluate_classifier(name=name, classifier_model= model, output_dir=\"test1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
